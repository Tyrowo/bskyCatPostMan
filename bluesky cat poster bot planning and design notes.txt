test the transformer model we found on huggingface to see if it can work
if not, run through the google colab
instead of letting it generate stuff try to steal images from bluesky?

https://huggingface.co/google/vit-base-patch16-224
google vision transformer come back and refresh on this once in a while so you can talk about it


maybe i should fine tune the ai model myself
no, I don't want to fine tune something myself I want to take advantage of some existing libraries


use firesky.tv hose of data to look for posts with [images]

need to get set of followed accounts
need to get database set of checked accounts


?? maybe ?? keep a set of accounts that have been screened - store these in a table that gets booted up every time 
maybe like a three strike policy? what's the odds that I check the same account more than once? guess it depends on how long it's been running

check if the account that posted the image is already being followed

now that we know it's a valid new account to check
run the image through an ai computer vision image classifier - determine whether or not the image is a cat.

?? maybe ?? if it is a cat check if there's text in the image


if the newly posted image is a cat then let's just assume they're a cat poster, like the image, follow the account, put in the database that we added this account for this post id and tested positive for cat = true


maybe we could have an account status monitoring - 
change in followers and follow count since last time ran,
check and see if any of the new followers were followed by the auto process so we can count gains from the bot



also need to hook it up to my google photos bucket of ricky pics and post a new one every day
what should the caption be?
:3, double heart, 


https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/feed/cats
cats feed to use

at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.feed.generator/whats-hot
this can replace the did -> cats part but you have to replace the app.feed.generator to just feed to use it in a url to see the page.

so if we use 
https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/app.bsky.feed.generator/cats
we should have a feed generator for our cats to be sourced from.

other cat feeds
caturday - https://bsky.app/profile/numb.comfortab.ly/feed/aaad4sb7tyvjw
handle - @numb.comfortab.ly

Siamese cats - https://bsky.app/profile/mrfenman.bsky.social/feed/aaac6wmikqyhq
handle - @mrfenman.bsky.social

cat pics - https://bsky.app/profile/jaz.bsky.social/feed/cv:cat
handle - @jaz.bsky.social 


db schema
table - followship
account | follow date | follow post | followed back | removed from friends
account id should be a primary key and indexed as a hash key 
| user_id | handle | follow_uri | follow_date | follows_you

max handle length is 300 chars so we're going to use that as max length for our varchars

table - cat checks
account | post # checked | was cat | was not text | followed?
account id should be a primary key and indexed as a hash key 
#nvm all that
| post id | seen_date | 

table - friend count and date
date (pk) | followers | following | engagement count on most recent 5 posts

because as of postgres 8.something it finally is WAL approved and works better than b-tree for its intended use case. When to consider using a hash index:
Primary key index: If you need extremely fast lookups on a unique primary key column. 
High cardinality, unique data: When most values in a column are distinct and you primarily perform equality comparisons. 
this is an exact description of what we need it for lol
however, we're going to be using Pandas to parse our data not the database itself, so we don't need all of that. 

Workflow 0 - initialize followership db:
for everyone in the friends list, add username, today's date, no post, and mark whether they are also following you, 
add all followed users to both tables

Workflow 1 - follow new catposters:
1. pull current posts from feed
2. check for posts with images
for each post {
3. see if the poster is already being followed/already was in db, if yes bail on this poster
4. run image through cat checker ai, if no bail on this poster
5. run image through text checker ai, if yes text bail on this poster
6. hooray! this is a real cat pic and not just a meme someone posted or something (hopefully). Follow the poster. Add username to the database of people, the date added, and the post, mark as false for followed by.
}

Workflow 2 - reducing follower/followed by ratio:
while followed by ratio is less than 35% (followed by / followed by + followers) {
1. scan through friends list, update relationships between followed by so that we don't delete anyone that has followed us back.
2. query the database for the last 10 people, are not following, and have not been removed from friends
3. mark them as removed from friends and unfollow them
}

Workflow 3 - posting:
# bluesky actually doesn't have native post scheduling which is very annoying because how am I supposed to post in bulk with a program you only run once without just clicking to run it every time you want to?
anyway...
# also need to figure out how to connect to the shared ricky album with the google photos api maybe? 
https://developers.google.com/photos/picker/guides/get-started-picker
otherwise we don't have any content to automatically post and this is a bust.
0. link ricky pics google drive folder
1. create set of pics and used pic ids
2. randomly select a pic, select one that hasn't been used already
3. post it.
4. mark as used
5. set up cron job for next time sometime random between 12 and 24 hours


step 5 - https://github.com/Zeeshanahmad4/Bluesky-Post-Bot 
details how to set up a cron schedule for GitHub posting. would need to make it private so people don't have access to my bluesky and google photos account, and it's kind of a security risk but whatever.


because of the google photos api complication that I don't want to deal with yet, let's get an mvp off the ground that only focuses on growth through followership, and I'll do the posting myself.
then once that's firmly working we can look into the other part. that'll give more time to think of hashtags and 


prioritized backlog - 

phase 1 - set up main ai photo recognition base and test bluesky functionality
1. set up colab
2. set up the bluesky code to get into the #list
2. figure out how to run that ai model
3. run both ai models on a sample of like 100 pics to get started
 - - have it pull 100 pics from today's view of the #list
4. see how the reverse image search works
5. set up liking pictures and following accounts on bluesky account
6. give it a spin

phase 2 - set up sql database
1. set up tables
2. initialize bsky friend analysis
3. add database updating to base bluesky functionality from phase 1
4. test 

phase 3 - friend pruning
1. create panda - sql queries to get updates to list - does it need to do this every time? oh yeah it's gotta check for follow backs.
-- with hash index lookup this is an o1 operation, and even if we have oN where n is our friend list size, going through even a billion friends wouldn't be that bad I think. So we can just do this every time.
2. calculate acceptable followership condition by given variable of x where is the % of followers to followers+following
3. [for if unacceptable ratio] write sql queries to get most recent x friends where x is the difference that will get us back up to a [acceptable]% follower ratio
4. use that query of people necessary to delete to get to correct ratio, and delete all the people from bluesky as necessary, when successful update table with new status, return all rows updated
5. test

phase 4 - logging and testing framework
1. write function to check current stats followers and following, need to check the most recent 5 posts and count up engagement. count a like as one engagement point and a repost as 50 engagement points.
2. update the other functions to run this stats updating in the relevant sql table when we run all the code
3. set up a bit of unit testing for each phase to show that it's all working correctly.

phase 5 - cron job automation
https://github.com/Zeeshanahmad4/Bluesky-Post-Bot
1. time to figure out how to get this process to automate so you can just have it run every night or something.
2. need to set up secret keys in GitHub and run it and see if everything is still working as intended.
3. get it to do everything fast one day to see it all in action, then set it to run.

phase 6 - dockerization???

phase 7 - LOGGING - 
1. automatic loggers that get posted to gethub project every time something gets run
1b. needs to show friends before action, friends after action, follows added/deleted

need to come up with way to go to second page - limit is 100 on feed requests.


post id, date checked
need to begin initialization with deletion of any posts older than three days old so we don't hold that



i'm trying hard to think about this workflow again
0. the database exists
update friends db?
1. pandas pulls our two tables of posts and ids, creating a set of s


I think we only care about our follows, not our followers - I don't really care how many bots have followed us in the previous week. 

follower ratio = len(client.get_follows('me').follows) / len(client.get_followers('me').followers)
following from get follows = client.get_follows('me').follows[x].following
see if they followback = client.get_follows('me').follows[x].followed_by


Pull up pandas, create map of user : follows_back
use client.get_follows('me') to pull up all users I am following
for each user in the get_follows results
	see if they are in the pandas set
	if they are not, add them to an add list with the information you need for the table
	if they are make sure that they still have the same follows_back status
		if they have changed add them to an update query list
after running through each user in the set 
run update query
run add query
run update query - any friends added that don't have an add date should be set to today


ok i can't figure out how to get the pyscopg2 connected because i can't figure out how to get the database set up to accept anything
eventually I'll have to ssh in
need to watch video on ssh even though you used to do it all the time you just did it for work without understanding a lot.
 

now that sql queries are getting generated successfully
need to {
  0. write queries to update friends list
  1. do deletion code
  2. do post database, and pull it into memory
  3. figure out what's causing the two-posts-same-did issue, maybe it takes a second for follower to go through?
	-- create a session set to check for
}

WE'VE OFFICIALLY DOUBLED!! - 2:21 pm 01/20/2025
jan 17 - jan 20 72 hrs
9am to 5:01pm 8 hrs

61 days = 1464 hours

487/1464 = 
0.3326502732240437 fol/hr

487/80 = 
6.0875

6.0875 - 0.3326502732240437
/ 0.3326502732240437
 = increased rate 17.3 * 100 = 1730%

really need to do figure out duplicate protection - here are some things I'm noticing. It's always just duplicates from the same query, it's never failing because the id already exists in the db, but because there are 2 duplicate ids in the query. So it simply must be because we can follow twice before the api 'realizes' we've already followed that user. Just need a temporary store of users for one query at a time, the issue is never that there is one user from a query we just did. although that would probably be better implementation just in case. How would we purge the set? well maybe just timeout is enough because we don't mind having that set open for infinity, we want all of our followslist in a set anyway.
OH I GET IT. the reason was because you make the query of all these feed posts at once, and while that is made it says that all of the users that aren't following you aren't following you. So when you follow one of the users, the feed pages that you've already loaded aren't updated. So the set fix is perfect.

the CATPICS feed is really not pulling it's weight lol could investigate some new feeds. Siamese is doing much better than I thought it would

deletion code without being able to pass the sql directly into python and parse with pandas is proving to be a tiny bit more of a chore than I'd hoped.
Luckily pruning doesn't occur very frequently, but for this immediate manual implementation I basically have to just have the query that gets all the follow IDs I want to remove, ctrl+H (replace) the at:// with , at://, and turn each line into an array to iterate through. 
Then you have to run another query to do the deletions.

Still need to write some code that will add users in that I have been adding through regular app usage. Similar to the initialization of the db code, but instead I need to write some to compare the db to the current friends list and make updates.
I think that part can wait until direct db connectivity is established to the code. Otherwise there's going to be a LOT of querying.

So for the deletion step
1. get list of friends in db with current friend status
2. search through follows and see if they follow you to update status
3. output update query with all users that need to be updated.

DELETE FROM "bsky"."follows"
WHERE user_handle = 'e1341.bsky.social'
RETURNING *;

DELETE FROM follows
WHERE follow_id IN (
    SELECT follow_id
    FROM follows
    WHERE follows_you = FALSE
    ORDER BY date_added ASC
    LIMIT 100
);


1/23/2025 - encountered my first error. Looks like someone deleted their picture or their whole post after I had generated the feed, and so when I tried to use the link to the image the source wasn't found and it caused a bad html request. So I added some very basic error handling.
But now that means I have ~50 new friends that aren't registered in the database.
It's time to create a function to update the friends list and see which followers have been missed.
This will probably be an update to the update friends function but definitely a little different. 
Need to pull the database somehow and register it as a... map? Maybe I need to actually use some pandas but building a dataframe out of text input seems like overkill.


need to rewrite the delete query to auto-parse the follow links instead of using notepad++ to add quotes.

part 4 needs to be implementing the post-cacheing feature, there's a lot of duplicate work being done checking posts from 1. earlier in the day and 2. posts that make it into multiple feeds.
The most important question is how big should our cache be? I guess I don't mind it growing to be about 5000, it would be the same as copying and pasting the friends list. Maybe before updating the cache you delete all but the last 3 days of posts? maybe even only 2 days?
for logging we could even set it up so that we input the post cid and the date we checked it, so when we log that we've already checked that post we can say if it was one that we saw this batch, or saw on x date. Normally that would be kind of overkill but here I think it's important to let us know how far back we need to save posts. Ultimately we might not even need to save more than like the past 12 hours. But without the timelogging there's no way to be sure.
That should be another statistic that we keep track of in the logging.

In fact, I need to start thinking about what kind of logging we want because that's the next thing to work on.
Stats we should log - Date of run, follow+follower count before. which feeds were checked, how many posts were checked. # posts with vids, # posts without pics, # errors, # pics processed, # cat pics observed, # people followed, # mutual posts seen, # already followed seen
total from all feeds run
For logging if the file gets above a certain size how do we initialize a new file?

then follow+followers after, ratio, and if we're still in acceptable territory
then if we're not it'll go through the deletion routine when we're fully automated, and it'll log that. 

should I actually log all of the handles and uris added? is that too much? or do we just want stats?

just had a weird error where I pulled up my own picture and tried to follow myself maybe? that's weird. Need to add some handling for that lol. To clarify it didn't error the program, but caused a sql error because I'm already in the database. But we're not pulling the db until friend pruning, only checking the follow and followback, so I guess we need a check to make sure that the user is not you in case you 
post a pic, and then start following people

great! logging works on the main add-followers part of the routine. now need to do logging for the updating followers and the deleting followers routines. 

could use a little effort in refactoring too - duplicated constants (datetime just got cloned)
constants moved up to the top?
once we get closer to full automation we'll need to do the full refactor, like making the queries prettier. so worry about that later, but get that datetime copy out

1/30/25 0100
the logging works great, and the caching works great! The performance improvement is incredibly obvious, being that it reduces the time from n*m to oN time, we no longer repeat reprocessing posts and that's like 9/10 of the time is processing the image.

The gains of followers obviously feels like it's slowing down. I think I'm still getting like 70 a day but it feels like a crawl. 
I have two ideas for how to improve this:
1. we create our own feed, and expand search terms. instead of just looking for cat, cats, Siamese, we look for all breeds, we look for words like kitty, kitten, and just trust our ai to weed out the nonsense.
2. more frequent but shallow friend searches. Instead of trying to do this twice a day at 2 and 8, maybe we need to do like 4 times a day at 08, 1300, 1700, 2100. We can just do a couple hundred but it'll be more likely to grab people that are actually on. 
The problem with this style is if we move forward with google cloud scheduling, they charge 10 cents per run, and this would be like 120 runs per month instead of 60. but maybe that's just a pay to play live with it kind of deal. or maybe we just set up the cron job on the laptop. Maybe I can rig it to an older laptop that just hangs out open all the time. 

now that logging and caching are complete though, we're getting to the point where all we have left to do is automate. So I think I need to start investigating connecting to the sql database from the internet now. Progress might be slowed, but at least we have the logging feature to feel good about and watch some progress for now, I wouldn't even mind taking a break to focus on some other work again for a week. 

Also side note on just the feelings of this project, it feels weird like I'm stressing about 70 followers gained in a day like it's not good enough. that's pretty insane actually.
I was worried like my picture today not reaching 100 likes meant I wasn't doing something right. But it has 90 likes that's awesome too.
I even made an unrelated mtg post just now and it already has 10 likes, better than almost any twitter post I ever made before this project.
The human condition normalizes the dopamine you get every step of the way and asks you to reach for more. But try to keep some perspective here. Everything is sweet.


The next direction I want to go is to explore replies to posts. Replies with cat pics are gold because that's the kind of user you want to be friends with. High engagement. 


2025-01-31 03:30 -
I realize now that if I'm not getting to the prefiously cached posts, then maybe I'm not reaching out to enough people. There must be a lot in the gaps that I'm not checking. And now it doesn't cost to put like 1000 posts for each feed, because any duplicates won't get checked.
I still think that 4 per day should be the way to go, but I should see how many it takes to get to ones that I've seen already
also should change logging to note diff between cached and seen posts.
2025-01-31 15:20 -
Went 1000 posts back this run, and it did not hit any cached posts from the run 12 hours ago.. So that means we can push back further. My goal for tomorrow (caturday) will be to check every post lol.
The one great part about the caching is if you change your mind and want to go back further than you just ran you won't hit those that you just did. Otherwise it seems very difficult lol.

Really need to implement reply reading next, like I was talking about before. that's gonna be huge.

2025-01-31 1630
I was scrolling through my Siamese cat feed when I saw a picture that perturbed me - a Siamese cat like ricky, no alt text, with 514 likes from yesterday. The user has less than half my followers and follows fewer people than that. 
But they do have 2400 posts, where I only have a few less than 200. They must be doing a LOT of replying.
My first instinct was to just go through and follow everyone who liked that cat post.
Maybe it's a mistake to try to follow Content creators. I want to get the people that ENJOY the kind of content I'm putting out.

What I need to do is find a way to do a graph traversal, whenever one of these cat pictures I encounter is popular, we have to go through all of the people liking those posts and start following THEM.
drum up more engagement on our own posts.

Also I want to create a function for the logging so that I can create a customized field where I can say like "we're updating our friends list today because I liked 500 users on my app instead of through rickybot"
but it has to be like... not permanent I don't want to accidentally set all of the logging functions to that so I won't do it for now.

But new goals
1. like likers
2. like replies

we still check if the post is in the cache
but before we skip it due to it being a mutual follower post or someone we already follow post check the number of likes that the post has.
	if it is over say 10? 25? then we check if it's a cat post regardless of if we follow that person or not and do like-based-following.
		but obviously if it's not a cat post then we skip it

This is a fundamental change in strategy, and though it will ruin the "timeline" because I won't have a perfectly cultured feed of cat posts forever, I will improve engagement.
Should I go to the cat-liker and like one of their posts before following? probably not. who knows what they're posting, after all.
but i guess if they show up in the cat feed then they'll get some mutual likes.

It's also time to discard other feeds. We only need Caturday + 1 cat type feed. Siamese Cats will be our primary.

Should write a query that groups entries in the follows table by the date_added and follows_you cols, so we can count the number of people from every batch that have and haven't followed
count follows you, count doesn't follow you, and % of followbacks from this batch

2025-02-01 01:01
I realize now that liking all likers of a post can lead to "friend bombs." 
I'm trying to keep a reasonable number of follows from posts and wasn't planning on capping follows from likes, but now i'm seeing posts with like THOUSANDS. I know I want more follows but how many more? hm.

2025-02-01 02:51

2500 Deletions got back to weds
need way to keep track of the db stats


2025-02-01 0423
ok maybe i pushed a little too far
got rate limited
so that's not good
but more importantly it totally borked the whole notebook and messed up my logging. Ran an update-db check to get the new users in. I was excited that my logging was working and I was watching the progress seeing all the ? emojis pop up. Very good visual warning lol. In the except clause I need to add a little more handling to bail out early if we go over like 10

"Under this system, an account may create at most 1,666 records per hour and 11,666 records per day. That means an account can like up to 1,666 records in one hour with no problem. We took the most active human users on the network into account when we set this threshold (you surpassed our expectations!)."
https://docs.bsky.app/docs/advanced-guides/rate-limits
Action Type	Value
CREATE	3 points
UPDATE	2 points
DELETE	1 point


2025-02-01
stop liking myself in the post likingfollows
since we have to make queries to get replies THAT is a reason to cap it to only popular posts, probably only worth it with like 25+ likers

2025-02-01 1220
690 worked no problem.
if you have 11666 and 1666 per hour that is 7 runs. if you have 7 runs in a day that is every 3.5 hours
so theoretically an optimal workload for the day would be <max amount of likes in 1666 points of CRUD> * 7 [at 0100, 0900, 1200, 1500, 1800, 2100, 2400]
now we just need to push to see what the max requests you can do in an hour really is.
1000 broke but I think that's because it was our second batch in an hour because I was testing code.
690 worked good this morning.
Next time we'll push to 1000 to see
I'm thinking that even if the liking posts drops down the rate limiting from like 1666 follows to only 1000 or 1500, that's still probably more than we can get from how many actual catposters we can follow in 3 hours, which would probably be like 500 maybe? But actually the reads are not create, update, or deletes so they shouldn't score any more points, it should still be 1666
hopefully the like queries are taking less than I thought and we can get pretty close to 1500 follows per batch.
I suppose instead of testing with 1000 next I should test 1500 next to make sure the rate limit is being handled ok now that I have that error handling break.
Let's stress test and do a full 1669 to make sure we hit the breaking point

Once we're habitually running maxxed out posts the cache will actually be useful, too! making sure we don't repeat work for real. I stopped pasting the queries in but I'll try it on this run that has only 2 hours of gap to see if it's useful in best case scenarios

looks like even if a post has 227 likes I'm only seeing the first 50. must be another page system, gotta fix that handling
tally up likers who were already followed/following and log that at the end of that function

ok wow the db cache actually worked a little on that stress test run

but we got 1201 new follows in our stress test run. not sure how many that was from liking posts and mutuals posts. 
need a successful run on the logs I accidentally forgot to put the error array in my join and blew it. next time though.
you really don't want to hit rate limit though, because then you can't like and respond to users on your phone. bad for engagement.
second stress test got 1140 new follows, definitely liked a lot more mutual posts because I was going through a lot of the ones I error-parsed last time.
The error handling is finally fixed and will successfully return out of the function and log now even if bad things happen.

when doing runs hour-to-hour the db cache was actually clutch. Extremely clutch. stopped processing like 6 pages of entries once we got towards the end.
which made our stress test not actually be so stressful, but that's a good thing. don't want to get rate limited for no reason lol.


ALWAYS NEED TO set up deletion day on Friday. Saturday should be adds only. We want to max growth on Saturday 100%, so that means no spending any of our CRUD point allowance on deletions.
1000 points on two thousand deletions was 333 follows we couldn't get to. but that's fine 3888 - 333 brings it down to 3555 for the day

finally got a successful stress test off!
error handling worked perfectly
1322 new users was max, with 32 mutual likes and 317 likes on the cat pics themselves
= 1671 actions. Pretty damn close to the the 1666 estimated.


automation - need to find some way to make sure we always have enough API power.
if we get a rate limit schedule update for an hour after, delete an hour after that. Or something, just gotta prevent jobs from running within an hour of each other.


how can I automate the statistics table? 
I want to pull this query to keep recording the percent of obtained followers at different times.

Time for a big combination of both documents.
I want to create a "settings" bar at the top 
how much I want to run the program, which feeds to run, whether or not to update, and whether or not to delete
The only thing is I really need to get database connectivity sorted out or I still have to do the copypasting for the cached posts and for the deletions. So annoying. 
If I could get the database connectivity I wouldn't even need to do copypasting at all. hm. Maybe I should reprioritize that.
Still, I think we almost always want to update our database shortly after running these queries so I'll at least bring that in and set some variables at the top in place of the parameters I keep changing.

2025-02-02 1607
when i was on a walk i realized that I do want to make the stats table. 
It should have date, # added total, # followbacks current, and followback% at sub2h, sub6h, sub12h, sub24h, sub48h, sub72h, sub1week, sub1month, and ending% After a week any non-followers will probably be purged, with our new higher rate I don't think people will last longer than a few days, really. when a date is purged then we get the final ending% value of followbacks. 


2025-02-03 0939
I keep thinking about this problem of automation, and I was wondering of if I should even have any self-hosted parts at all. Perhaps it would be better for my development as a developer to put the aws practices I'm learning into effect instead of the sql ones.
I was looking at this service that lets you automate a jupyter notebook, but I keep trying to do the math on the runtime 

Maybe I need to use the timer magic on one page of 100 posts in a feed. run that as many times as possible and see how much runtime it takes. 
Or maybe my logging I can add another timer to the logging before it posts so I can see how much cpu time it will take.

Regardless I'm thinking that maybe if I containerize the function to preserve the bsky and ml imports, we can turn this script into a lambda function on aws so that it's easy to automate.
Then if I used dynamodb instead of aws rds I could be entirely serverless and that would be pretty cool.
The only thing is it would discard the database components as they are in favor of a NoSQL version.
And that would require a bit of refactoring, because I've learned that dynamodb runs on apis to delete key-value pairs, and that the max you can do in a batch is 25, so it isn't really prepared to process huge loads of multiple thousands.

But here's how I'm thinking we refactor.
Instead of storing the entire database of friends in dynamodb, we store RUNS in dynamodb. we have a key which is the date, and the value is an array of dids that we added during that run. A set amount of days after that run completes, we pull that entry's array, check every follower to see if they're following now. and if they're not, delete them. Pruning based on batch date rather than on a query to get the oldest users from the db.
The only things that we would have to account for better is 1. getting human-added users into a batch somehow, and 2. picking up on the random one-offs where they followed during that first week but then later unfollowed.
Maybe like once a week we have a full overhaul of the friends list and see how everything is doing? A super-batch of sorts. Every week we create a superbatch of every follow we have, and then the following week we see how that went and create a new superbatch of what's left next. 
If we're doing that we almost don't need to worry about the other days as much.
Maybe one deletion update can happen every night to cover the follows from exactly 7 days prior.

Migrating to AWS will definitely solve the issue of our self-hosted db communication issue. 
But to implement it first we need to figure out if this containerized deployment of the routine to a lambda function will work at all. So I'll try to get what we've got put into a lambda function for now. Maybe the lambda function can email me the outputs with amazon sns so I can paste the queries in. 
We'll focus on automating there, and then once we know that we CAN do it that way, I'll incorporate dynamodb and we'll redo the lambda once that works.

i think that this kind of solution should scale better. We know our max scaling is capped by the bluesky api rate limiting now. 
Caturday is a power day, and the other days can take it easy. That way we can average out our usage over the course of the month (aws bills by month).

this would also solve the issue of rate limits being encountered by deletions too. It takes much longer, looks like about 3930 instead of 1700 (not sure how the rate limiting math works there, should still be 2 points instead of 3 but i guess without the post liking that adds up a lot) but I managed to accidentally stress test without thinking on 4500.
Curious what happens when we run the same batch of deletions in an hour when our rate limit unexpires but we've already half-processed them.
Suffice to say we should only delete 3500 users in a batch max in the future from now on. Added error handling to the deletion part too though so that's good.

Well I think the program didn't actually fail due to rate limiting, there was actually a None value in the list that caused an error at 3930, exactly where our program stopped. I left the None value in this time and it appropriately caught the error. 
I do need to fix the logging to somehow see how many people we actually deleted though.

And also need to add a second logging time to the end to see the end times for everything! Gotta start measuring time.

So to solve the dynamodb problem for the other table, posts, the posts cache.
what we'll store is just a single row, which is the posts we encountered in our last post. That way the size doesn't grow exponentially, it only is one read and one write every time, and we store it pre-parsed into an array already. Ezpz


this run of 403 posts 691 follows took 7:00.2087, which is 420 seconds.

aws gives 3.2 million seconds of compute time per month free
gb of compute time is measured by how much memory you're using too, so I need to start tracking the memory of my program so I can get an impression of how it's growing per follow+posts

2025-02-03 2209
OHH! I just had a brilliant idea of how to handle some of the stragglers. If someone unfollows we shouldn't keep them in the pool and reintroduce them to a new batch, instead we should add those people to the delete query and just unfollow them ourselves because there's no way they're coming back. That means all we need to account for is the people we're adding manually.

maybe it's backwards to containerize and then set it up to work with dynamodb and then recontainerize.
dynamodb should be an easy hookup now that we have things planned out.

2025-02-07 0149
been taking a break from updating the program so that I can focus on learning AWS concepts, primarily lambdas and how to dockerize this project so we can automate that way.
Will need to create some memory benchmarks so that we can get an initial impression of how much memory our program needs. We might be able to do it through playing around though, Mostly assuming that we'll be comparing 1gb vs 2gb. Not really worried about the memory limits itself, but rather that the cpu allocated to us will be better with higher mem and that might help run the computer vis.

I'm also thinking about the math of how we should be running it.
Since we don't really need to limit the cron jobs anymore, we could do as much as 1 every half hour if we really wanted to. I'm not a fan of that idea because I think it will cause too much overlap.
I think 1 per hour on caturdays, and one every 2 hours on not-caturdays.
Will need to look how EventBridge does scheduled asynchronous invocations for the lambda function, and see what kind of info I can pass in there. Perhaps it will be able to say what feed we're using, but might need to just handle that in code the way I'm doing it now. 

so let's think about maxes
11666 per day, 1666 per hour. 1322 new users was max, with 32 mutual likes and 317 likes on the cat pics themselves
= 1671 actions
11666/1666 = 7.002400 MAXED OUT runs per day. So if we want to do 4 runs per day we could simply divide the max out by 4, which should give us enough of a buffer since 24/7 is ~ 3.5
instead of looking for 1322 follows, we'll look for 300, which will also give us plenty of buffer for likes.
So every hour on caturday we do 300 follows, which will gain us 7200 follows.
every day except Saturday we'll have a deletion time that gets rid of everything from older than one week prior.
non caturdays we should do maybe like 250 every 2 hours? that's 3000 per day and should give us plenty of room to do our deletions. Friday will be sketchier, but we just need to schedule the deletions on an off-hour
