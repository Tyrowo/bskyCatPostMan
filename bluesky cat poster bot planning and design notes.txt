test the transformer model we found on huggingface to see if it can work
if not, run through the google colab
instead of letting it generate stuff try to steal images from bluesky?

https://huggingface.co/google/vit-base-patch16-224
google vision transformer come back and refresh on this once in a while so you can talk about it


maybe i should fine tune the ai model myself
no, I don't want to fine tune something myself I want to take advantage of some existing libraries


use firesky.tv hose of data to look for posts with [images]

need to get set of followed accounts
need to get database set of checked accounts


?? maybe ?? keep a set of accounts that have been screened - store these in a table that gets booted up every time 
maybe like a three strike policy? what's the odds that I check the same account more than once? guess it depends on how long it's been running

check if the account that posted the image is already being followed

now that we know it's a valid new account to check
run the image through an ai computer vision image classifier - determine whether or not the image is a cat.

?? maybe ?? if it is a cat check if there's text in the image


if the newly posted image is a cat then let's just assume they're a cat poster, like the image, follow the account, put in the database that we added this account for this post id and tested positive for cat = true


maybe we could have an account status monitoring - 
change in followers and follow count since last time ran,
check and see if any of the new followers were followed by the auto process so we can count gains from the bot



also need to hook it up to my google photos bucket of ricky pics and post a new one every day
what should the caption be?
:3, double heart, 


https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/feed/cats
cats feed to use

at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.feed.generator/whats-hot
this can replace the did -> cats part but you have to replace the app.feed.generator to just feed to use it in a url to see the page.

so if we use 
https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/app.bsky.feed.generator/cats
we should have a feed generator for our cats to be sourced from.

other cat feeds
caturday - https://bsky.app/profile/numb.comfortab.ly/feed/aaad4sb7tyvjw
handle - @numb.comfortab.ly

Siamese cats - https://bsky.app/profile/mrfenman.bsky.social/feed/aaac6wmikqyhq
handle - @mrfenman.bsky.social

cat pics - https://bsky.app/profile/jaz.bsky.social/feed/cv:cat
handle - @jaz.bsky.social 


db schema
table - followship
account | follow date | follow post | followed back | removed from friends
account id should be a primary key and indexed as a hash key 
| user_id | handle | follow_uri | follow_date | follows_you

max handle length is 300 chars so we're going to use that as max length for our varchars

table - cat checks
account | post # checked | was cat | was not text | followed?
account id should be a primary key and indexed as a hash key 
#nvm all that
| post id | seen_date | 

table - friend count and date
date (pk) | followers | following | engagement count on most recent 5 posts

because as of postgres 8.something it finally is WAL approved and works better than b-tree for its intended use case. When to consider using a hash index:
Primary key index: If you need extremely fast lookups on a unique primary key column. 
High cardinality, unique data: When most values in a column are distinct and you primarily perform equality comparisons. 
this is an exact description of what we need it for lol
however, we're going to be using Pandas to parse our data not the database itself, so we don't need all of that. 

Workflow 0 - initialize followership db:
for everyone in the friends list, add username, today's date, no post, and mark whether they are also following you, 
add all followed users to both tables

Workflow 1 - follow new catposters:
1. pull current posts from feed
2. check for posts with images
for each post {
3. see if the poster is already being followed/already was in db, if yes bail on this poster
4. run image through cat checker ai, if no bail on this poster
5. run image through text checker ai, if yes text bail on this poster
6. hooray! this is a real cat pic and not just a meme someone posted or something (hopefully). Follow the poster. Add username to the database of people, the date added, and the post, mark as false for followed by.
}

Workflow 2 - reducing follower/followed by ratio:
while followed by ratio is less than 35% (followed by / followed by + followers) {
1. scan through friends list, update relationships between followed by so that we don't delete anyone that has followed us back.
2. query the database for the last 10 people, are not following, and have not been removed from friends
3. mark them as removed from friends and unfollow them
}

Workflow 3 - posting:
# bluesky actually doesn't have native post scheduling which is very annoying because how am I supposed to post in bulk with a program you only run once without just clicking to run it every time you want to?
anyway...
# also need to figure out how to connect to the shared ricky album with the google photos api maybe? 
https://developers.google.com/photos/picker/guides/get-started-picker
otherwise we don't have any content to automatically post and this is a bust.
0. link ricky pics google drive folder
1. create set of pics and used pic ids
2. randomly select a pic, select one that hasn't been used already
3. post it.
4. mark as used
5. set up cron job for next time sometime random between 12 and 24 hours


step 5 - https://github.com/Zeeshanahmad4/Bluesky-Post-Bot 
details how to set up a cron schedule for GitHub posting. would need to make it private so people don't have access to my bluesky and google photos account, and it's kind of a security risk but whatever.


because of the google photos api complication that I don't want to deal with yet, let's get an mvp off the ground that only focuses on growth through followership, and I'll do the posting myself.
then once that's firmly working we can look into the other part. that'll give more time to think of hashtags and 


prioritized backlog - 

phase 1 - set up main ai photo recognition base and test bluesky functionality
1. set up colab
2. set up the bluesky code to get into the #list
2. figure out how to run that ai model
3. run both ai models on a sample of like 100 pics to get started
 - - have it pull 100 pics from today's view of the #list
4. see how the reverse image search works
5. set up liking pictures and following accounts on bluesky account
6. give it a spin

phase 2 - set up sql database
1. set up tables
2. initialize bsky friend analysis
3. add database updating to base bluesky functionality from phase 1
4. test 

phase 3 - friend pruning
1. create panda - sql queries to get updates to list - does it need to do this every time? oh yeah it's gotta check for follow backs.
-- with hash index lookup this is an o1 operation, and even if we have oN where n is our friend list size, going through even a billion friends wouldn't be that bad I think. So we can just do this every time.
2. calculate acceptable followership condition by given variable of x where is the % of followers to followers+following
3. [for if unacceptable ratio] write sql queries to get most recent x friends where x is the difference that will get us back up to a [acceptable]% follower ratio
4. use that query of people necessary to delete to get to correct ratio, and delete all the people from bluesky as necessary, when successful update table with new status, return all rows updated
5. test

phase 4 - logging and testing framework
1. write function to check current stats followers and following, need to check the most recent 5 posts and count up engagement. count a like as one engagement point and a repost as 50 engagement points.
2. update the other functions to run this stats updating in the relevant sql table when we run all the code
3. set up a bit of unit testing for each phase to show that it's all working correctly.

phase 5 - cron job automation
https://github.com/Zeeshanahmad4/Bluesky-Post-Bot
1. time to figure out how to get this process to automate so you can just have it run every night or something.
2. need to set up secret keys in GitHub and run it and see if everything is still working as intended.
3. get it to do everything fast one day to see it all in action, then set it to run.

phase 6 - dockerization???

phase 7 - LOGGING - 
1. automatic loggers that get posted to gethub project every time something gets run
1b. needs to show friends before action, friends after action, follows added/deleted

need to come up with way to go to second page - limit is 100 on feed requests.


post id, date checked
need to begin initialization with deletion of any posts older than three days old so we don't hold that



i'm trying hard to think about this workflow again
0. the database exists
update friends db?
1. pandas pulls our two tables of posts and ids, creating a set of s


I think we only care about our follows, not our followers - I don't really care how many bots have followed us in the previous week. 

follower ratio = len(client.get_follows('me').follows) / len(client.get_followers('me').followers)
following from get follows = client.get_follows('me').follows[x].following
see if they followback = client.get_follows('me').follows[x].followed_by


Pull up pandas, create map of user : follows_back
use client.get_follows('me') to pull up all users I am following
for each user in the get_follows results
	see if they are in the pandas set
	if they are not, add them to an add list with the information you need for the table
	if they are make sure that they still have the same follows_back status
		if they have changed add them to an update query list
after running through each user in the set 
run update query
run add query
run update query - any friends added that don't have an add date should be set to today


ok i can't figure out how to get the pyscopg2 connected because i can't figure out how to get the database set up to accept anything
eventually I'll have to ssh in
need to watch video on ssh even though you used to do it all the time you just did it for work without understanding a lot.
 

now that sql queries are getting generated successfully
need to {
  0. write queries to update friends list
  1. do deletion code
  2. do post database, and pull it into memory
  3. figure out what's causing the two-posts-same-did issue, maybe it takes a second for follower to go through?
	-- create a session set to check for
}

WE'VE OFFICIALLY DOUBLED!! - 2:21 pm 01/20/2025
jan 17 - jan 20 72 hrs
9am to 5:01pm 8 hrs

61 days = 1464 hours

487/1464 = 
0.3326502732240437 fol/hr

487/80 = 
6.0875

6.0875 - 0.3326502732240437
/ 0.3326502732240437
 = increased rate 17.3 * 100 = 1730%

really need to do figure out duplicate protection - here are some things I'm noticing. It's always just duplicates from the same query, it's never failing because the id already exists in the db, but because there are 2 duplicate ids in the query. So it simply must be because we can follow twice before the api 'realizes' we've already followed that user. Just need a temporary store of users for one query at a time, the issue is never that there is one user from a query we just did. although that would probably be better implementation just in case. How would we purge the set? well maybe just timeout is enough because we don't mind having that set open for infinity, we want all of our followslist in a set anyway.
OH I GET IT. the reason was because you make the query of all these feed posts at once, and while that is made it says that all of the users that aren't following you aren't following you. So when you follow one of the users, the feed pages that you've already loaded aren't updated. So the set fix is perfect.

the CATPICS feed is really not pulling it's weight lol could investigate some new feeds. Siamese is doing much better than I thought it would

deletion code without being able to pass the sql directly into python and parse with pandas is proving to be a tiny bit more of a chore than I'd hoped.
Luckily pruning doesn't occur very frequently, but for this immediate manual implementation I basically have to just have the query that gets all the follow IDs I want to remove, ctrl+H (replace) the at:// with , at://, and turn each line into an array to iterate through. 
Then you have to run another query to do the deletions.

Still need to write some code that will add users in that I have been adding through regular app usage. Similar to the initialization of the db code, but instead I need to write some to compare the db to the current friends list and make updates.
I think that part can wait until direct db connectivity is established to the code. Otherwise there's going to be a LOT of querying.

So for the deletion step
1. get list of friends in db with current friend status
2. search through follows and see if they follow you to update status
3. output update query with all users that need to be updated.

DELETE FROM "bsky"."follows"
WHERE user_handle = 'e1341.bsky.social'
RETURNING *;

DELETE FROM follows
WHERE follow_id IN (
    SELECT follow_id
    FROM follows
    WHERE follows_you = FALSE
    ORDER BY date_added ASC
    LIMIT 100
);


1/23/2025 - encountered my first error. Looks like someone deleted their picture or their whole post after I had generated the feed, and so when I tried to use the link to the image the source wasn't found and it caused a bad html request. So I added some very basic error handling.
But now that means I have ~50 new friends that aren't registered in the database.
It's time to create a function to update the friends list and see which followers have been missed.
This will probably be an update to the update friends function but definitely a little different. 
Need to pull the database somehow and register it as a... map? Maybe I need to actually use some pandas but building a dataframe out of text input seems like overkill.


need to rewrite the delete query to auto-parse the follow links instead of using notepad++ to add quotes.

part 4 needs to be implementing the post-cacheing feature, there's a lot of duplicate work being done checking posts from 1. earlier in the day and 2. posts that make it into multiple feeds.
The most important question is how big should our cache be? I guess I don't mind it growing to be about 5000, it would be the same as copying and pasting the friends list. Maybe before updating the cache you delete all but the last 3 days of posts? maybe even only 2 days?
for logging we could even set it up so that we input the post cid and the date we checked it, so when we log that we've already checked that post we can say if it was one that we saw this batch, or saw on x date. Normally that would be kind of overkill but here I think it's important to let us know how far back we need to save posts. Ultimately we might not even need to save more than like the past 12 hours. But without the timelogging there's no way to be sure.
That should be another statistic that we keep track of in the logging.

In fact, I need to start thinking about what kind of logging we want because that's the next thing to work on.
Stats we should log - Date of run, follow+follower count before. which feeds were checked, how many posts were checked. # posts with vids, # posts without pics, # errors, # pics processed, # cat pics observed, # people followed, # mutual posts seen, # already followed seen
total from all feeds run
For logging if the file gets above a certain size how do we initialize a new file?

then follow+followers after, ratio, and if we're still in acceptable territory
then if we're not it'll go through the deletion routine when we're fully automated, and it'll log that. 

should I actually log all of the handles and uris added? is that too much? or do we just want stats?

just had a weird error where I pulled up my own picture and tried to follow myself maybe? that's weird. Need to add some handling for that lol. To clarify it didn't error the program, but caused a sql error because I'm already in the database. But we're not pulling the db until friend pruning, only checking the follow and followback, so I guess we need a check to make sure that the user is not you in case you 
post a pic, and then start following people

great! logging works on the main add-followers part of the routine. now need to do logging for the updating followers and the deleting followers routines. 

could use a little effort in refactoring too - duplicated constants (datetime just got cloned)
constants moved up to the top?
once we get closer to full automation we'll need to do the full refactor, like making the queries prettier. so worry about that later, but get that datetime copy out

1/30/25 0100
the logging works great, and the caching works great! The performance improvement is incredibly obvious, being that it reduces the time from n*m to oN time, we no longer repeat reprocessing posts and that's like 9/10 of the time is processing the image.

The gains of followers obviously feels like it's slowing down. I think I'm still getting like 70 a day but it feels like a crawl. 
I have two ideas for how to improve this:
1. we create our own feed, and expand search terms. instead of just looking for cat, cats, Siamese, we look for all breeds, we look for words like kitty, kitten, and just trust our ai to weed out the nonsense.
2. more frequent but shallow friend searches. Instead of trying to do this twice a day at 2 and 8, maybe we need to do like 4 times a day at 08, 1300, 1700, 2100. We can just do a couple hundred but it'll be more likely to grab people that are actually on. 
The problem with this style is if we move forward with google cloud scheduling, they charge 10 cents per run, and this would be like 120 runs per month instead of 60. but maybe that's just a pay to play live with it kind of deal. or maybe we just set up the cron job on the laptop. Maybe I can rig it to an older laptop that just hangs out open all the time. 

now that logging and caching are complete though, we're getting to the point where all we have left to do is automate. So I think I need to start investigating connecting to the sql database from the internet now. Progress might be slowed, but at least we have the logging feature to feel good about and watch some progress for now, I wouldn't even mind taking a break to focus on some other work again for a week. 

Also side note on just the feelings of this project, it feels weird like I'm stressing about 70 followers gained in a day like it's not good enough. that's pretty insane actually.
I was worried like my picture today not reaching 100 likes meant I wasn't doing something right. But it has 90 likes that's awesome too.
I even made an unrelated mtg post just now and it already has 10 likes, better than almost any twitter post I ever made before this project.
The human condition normalizes the dopamine you get every step of the way and asks you to reach for more. But try to keep some perspective here. Everything is sweet.


The next direction I want to go is to explore replies to posts. Replies with cat pics are gold because that's the kind of user you want to be friends with. High engagement. 


2025-01-31 03:30 -
I realize now that if I'm not getting to the prefiously cached posts, then maybe I'm not reaching out to enough people. There must be a lot in the gaps that I'm not checking. And now it doesn't cost to put like 1000 posts for each feed, because any duplicates won't get checked.
I still think that 4 per day should be the way to go, but I should see how many it takes to get to ones that I've seen already
also should change logging to note diff between cached and seen posts.
2025-01-31 15:20 -
Went 1000 posts back this run, and it did not hit any cached posts from the run 12 hours ago.. So that means we can push back further. My goal for tomorrow (caturday) will be to check every post lol.
The one great part about the caching is if you change your mind and want to go back further than you just ran you won't hit those that you just did. Otherwise it seems very difficult lol.

Really need to implement reply reading next, like I was talking about before. that's gonna be huge.

2025-01-31 1630
I was scrolling through my Siamese cat feed when I saw a picture that perturbed me - a Siamese cat like ricky, no alt text, with 514 likes from yesterday. The user has less than half my followers and follows fewer people than that. 
But they do have 2400 posts, where I only have a few less than 200. They must be doing a LOT of replying.
My first instinct was to just go through and follow everyone who liked that cat post.
Maybe it's a mistake to try to follow Content creators. I want to get the people that ENJOY the kind of content I'm putting out.

What I need to do is find a way to do a graph traversal, whenever one of these cat pictures I encounter is popular, we have to go through all of the people liking those posts and start following THEM.
drum up more engagement on our own posts.

Also I want to create a function for the logging so that I can create a customized field where I can say like "we're updating our friends list today because I liked 500 users on my app instead of through rickybot"
but it has to be like... not permanent I don't want to accidentally set all of the logging functions to that so I won't do it for now.

But new goals
1. like likers
2. like replies

we still check if the post is in the cache
but before we skip it due to it being a mutual follower post or someone we already follow post check the number of likes that the post has.
	if it is over say 10? 25? then we check if it's a cat post regardless of if we follow that person or not and do like-based-following.
		but obviously if it's not a cat post then we skip it

This is a fundamental change in strategy, and though it will ruin the "timeline" because I won't have a perfectly cultured feed of cat posts forever, I will improve engagement.
Should I go to the cat-liker and like one of their posts before following? probably not. who knows what they're posting, after all.
but i guess if they show up in the cat feed then they'll get some mutual likes.

It's also time to discard other feeds. We only need Caturday + 1 cat type feed. Siamese Cats will be our primary.

Should write a query that groups entries in the follows table by the date_added and follows_you cols, so we can count the number of people from every batch that have and haven't followed
count follows you, count doesn't follow you, and % of followbacks from this batch

2025-02-01 01:01
I realize now that liking all likers of a post can lead to "friend bombs." 
I'm trying to keep a reasonable number of follows from posts and wasn't planning on capping follows from likes, but now i'm seeing posts with like THOUSANDS. I know I want more follows but how many more? hm.


