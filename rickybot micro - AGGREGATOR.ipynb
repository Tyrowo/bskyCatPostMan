{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRt2K0KE3DcHZzymuVV5eF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1tnU0vFn4Js","executionInfo":{"status":"ok","timestamp":1739763944495,"user_tz":300,"elapsed":2874,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}},"outputId":"f424ade3-cf4e-4b91-83e9-d700988f2a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.36.21)\n","Requirement already satisfied: botocore<1.37.0,>=1.36.21 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.36.21)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.11.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.0,>=1.36.21->boto3) (2.8.2)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.0,>=1.36.21->boto3) (2.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.21->boto3) (1.17.0)\n"]}],"source":["pip install boto3"]},{"cell_type":"code","source":["# import colab secrets to store login credentials\n","from google.colab import userdata\n","\n","# aws stuff\n","import boto3\n","from botocore.exceptions import ClientError\n","\n","# json necessary to parse secret string, and write/read s3 objects\n","import json\n","\n","# datetime is necessary for our ddb and s3 schema\n","import datetime\n","import zoneinfo\n","\n","# for logging\n","import base64\n","import requests"],"metadata":{"id":"aRNlTiVioavN","executionInfo":{"status":"ok","timestamp":1739763944495,"user_tz":300,"elapsed":3,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["AWS_KEY = userdata.get('aws_access_key')\n","AWS_SECRET_KEY = userdata.get('aws_secret_access_key')\n","REGION = userdata.get('aws_region')\n","SECRETS_ID = userdata.get('aws_secretsmanager_id')\n","\n","DDB = 'dynamodb'\n","S3 = 's3'\n","DDB_TABLE = 'rickybot-ddb'\n","S3_BUCKET = 'rickybot-s3'\n","\n","PRIMARY_KEY = 'DOW' # the dynamodb table's primary key. there is no sort key\n","DOW_KEYS = {\n","    'Sunday': 'SUN',\n","    'Monday': 'MON',\n","    'Tuesday': 'TUE',\n","    'Wednesday': 'WED',\n","    'Thursday': 'THU',\n","    'Friday': 'FRI+SAT',\n","    'Saturday': 'FRI+SAT'\n","}\n","USER_TIMEZONE = \"US/Eastern\"\n","\n","FILE_PATH = \"LOGGING_AGG_01.txt\"\n","BRANCH = \"main\""],"metadata":{"id":"7Jo0wvnFodYN","executionInfo":{"status":"ok","timestamp":1739763946016,"user_tz":300,"elapsed":1524,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# get the day of the week so we know what dynamodb key to pull from and which bucket to aggregate to\n","# doing this first because we do not run this on saturday and can bail out early if we get into this code for some reason\n","# also we are running this at about 1am, the following day after all runs have concluded for the previous. so we're aggregating the previous day's results\n","cur_timestamp = datetime.datetime.now(zoneinfo.ZoneInfo(USER_TIMEZONE))\n","yest_timestamp = cur_timestamp - datetime.timedelta(days=1)\n","yesterday = yest_timestamp.strftime(\"%A\")\n","\n","if yesterday == 'Friday':\n","  print(\"it's saturday, you shouldn't be here.\")\n","  # return early\n","\n","# use the day of the week to pull up the corresponding key for our dynamodb entries and our s3 bucket\n","ddbs3_key = DOW_KEYS[yesterday]\n","print(ddbs3_key)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tS1P5pvb1-Nz","executionInfo":{"status":"ok","timestamp":1739768695863,"user_tz":300,"elapsed":106,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}},"outputId":"41901e2b-f1de-446c-d99e-1a5b114a02a7"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["SUN\n","2025-02-17 00:04:55.437591-05:00\n"]}]},{"cell_type":"code","source":["# connect to aws\n","try:\n","  aws_session = boto3.Session(\n","          aws_access_key_id = AWS_KEY,\n","          aws_secret_access_key = AWS_SECRET_KEY,\n","          region_name = REGION\n","      )\n","except:\n","  print('failed to begin AWS session')\n","  # return with error\n","  # this is the only error that we can't log to github, because we never got the credentials"],"metadata":{"id":"rfMLg8ztoiAy","executionInfo":{"status":"ok","timestamp":1739763946017,"user_tz":300,"elapsed":4,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# then connect to secrets manager\n","try:\n","  secrets_client = aws_session.client('secretsmanager')\n","  secret_value = secrets_client.get_secret_value(SecretId=SECRETS_ID)\n","  secret_string = secret_value['SecretString']\n","  secret_map = json.loads(secret_string)\n","except:\n","  print('failed to reach aws secrets manager')\n","  # return with error"],"metadata":{"id":"uGbInUUHoihx","executionInfo":{"status":"ok","timestamp":1739763946581,"user_tz":300,"elapsed":568,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# create constants from the values in the secrets manager\n","BSKY_USERNAME = secret_map['bsky_username']\n","BSKY_PASS = secret_map['bsky_password']\n","GITHUB_TOKEN = secret_map['github_token']\n","GITHUB_REPO = secret_map['github_user/repo']\n","HUGGING_TOKEN = secret_map['hugging_token']"],"metadata":{"id":"AIbAuI8koq5b","executionInfo":{"status":"ok","timestamp":1739763946581,"user_tz":300,"elapsed":2,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# before the program starts let's set up the logging function so we can insert it at any point where our program could break\n","def logging_aggregator(logging_text):\n","  # LOGGING ALL THE CHANGES TO OUR LOGGING FILE IN GITHUB\n","  datetime_object = datetime.datetime.fromtimestamp(cur_timestamp.timestamp())\n","  date_only = str(datetime_object.date())\n","  commit_message = \"Logging follow aggregation on \" + date_only\n","\n","\n","  # Step 1: Get the file's current content and SHA\n","  url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/{FILE_PATH}\"\n","  headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n","  response = requests.get(url, headers=headers)\n","  response_json = response.json()\n","\n","  # Decode the content of the file\n","  file_sha = response_json[\"sha\"]\n","  content = base64.b64decode(response_json[\"content\"]).decode(\"utf-8\")\n","\n","  # Step 2: Modify the file content\n","  new_content = content + date_only + ': ' + logging_text + '\\n'\n","  encoded_content = base64.b64encode(new_content.encode(\"utf-8\")).decode(\"utf-8\")\n","\n","  # Step 3: Push the updated content\n","  data = {\n","      \"message\": commit_message,\n","      \"content\": encoded_content,\n","      \"sha\": file_sha,\n","      \"branch\": BRANCH,\n","  }\n","  update_response = requests.put(url, headers=headers, json=data)\n","\n","  if update_response.status_code == 200:\n","      print(\"Logging file updated successfully! Here's what was added to the logs:\")\n","      print(date_only + \": \" + logging_text)\n","  else:\n","      print(f\"Error: {update_response.json()}\")"],"metadata":{"id":"jmnfsI6-Q-Do","executionInfo":{"status":"ok","timestamp":1739842637081,"user_tz":300,"elapsed":145,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# initialize dynamodb and s3\n","try:\n","  dynamodb = aws_session.resource(DDB)\n","  table = dynamodb.Table(DDB_TABLE)\n","except:\n","  print('ERROR - failed to get dynamo db table')\n","  logging_aggregator('ERROR - failed to get dynamo db table')\n","  # return with error\n","try:\n","  s3 = aws_session.client(S3)\n","  buckets = s3.list_buckets()\n","  bucket = s3.list_objects_v2(Bucket=S3_BUCKET)\n","except:\n","  print('ERROR - failed to get s3 bucket')\n","  logging_aggregator('ERROR - failed to get s3 bucket')\n","  # return with error"],"metadata":{"id":"JygplaDi09rn","executionInfo":{"status":"ok","timestamp":1739763947860,"user_tz":300,"elapsed":1280,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# keep track of everything in a set so we don't have duplicates\n","follows_aggregation = set()"],"metadata":{"id":"y9eDMou56vPb","executionInfo":{"status":"ok","timestamp":1739763949047,"user_tz":300,"elapsed":160,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# there should NOT be anything in the current s3 object for this bucket. But just in case there is, like one week didn't properly get cleared out or something, we will add it to the beginning of the aggregation\n","try:\n","  s3.head_object(Bucket=S3_BUCKET, Key=ddbs3_key)\n","  print(\"WARNING - Object existed in s3 bucket. Aggregating to current results.\")\n","  logging_aggregator(\"WARNING - Object existed in s3 bucket. Aggregating to current results.\")\n","  response = s3.get_object(Bucket=S3_BUCKET, Key=ddbs3_key)\n","  # creates a list from the json info in the s3 bucket\n","  data = json.loads(response[\"Body\"].read())\n","  # add all items from the list into our current set\n","  follows_aggregation.update(data)\n","except s3.exceptions.ClientError as e:\n","  if e.response[\"Error\"][\"Code\"] == \"404\":\n","      print(\"Clear to proceed - object did not exist in s3 bucket\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaSx_0Tg1pBk","executionInfo":{"status":"ok","timestamp":1739763953217,"user_tz":300,"elapsed":133,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}},"outputId":"e7fa7138-b340-4bd5-a17f-33d03e3ef6cc"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Clear to proceed - object did not exist in s3 bucket\n"]}]},{"cell_type":"code","source":["# now it's time to iterate through our the attributes on our dynamodb key\n","try:\n","  ddb_response = table.get_item(\n","      Key={'DOW': ddbs3_key},\n","  )\n","except ClientError as e:\n","  print(f\"ERROR - failed to check key's existence: {e}\")\n","  logging_aggregator(f\"ERROR - failed to check key's existence: {e}\")\n","\n","print('ddb response:', ddb_response)\n","# this if else checks to see if there is anything\n","if 'Item' not in ddb_response:\n","  print('WARNING - found no items in this key, runs may have failed yesterday')\n","  logging_aggregator('WARNING - found no items in this key, runs may have failed yesterday')\n","else:\n","  for attribute in ddb_response['Item']: # this iterates through all the attributes in the key\n","    # for val in ddb_response['Item'][attribute]: # and this iterates through all of the values in the value of that key\n","    # instead of iterating through all the values we'll just add them all into the set directly\n","    # print('attr', ddb_response['Item'][attribute])\n","    follows_aggregation.update(ddb_response['Item'][attribute])\n","\n","  # after finishing iterating through all of the attributes we can delete this key from the dynamodb to clear out all the previous runs\n","  try:\n","    table.delete_item(\n","      Key={'DOW': ddbs3_key}\n","    )\n","  except ClientError as e:\n","    print(f\"ERROR - failed to delete item {ddbs3_key} from dynamodb: {e}\")\n","    logging_aggregator(f\"ERROR - failed to delete item {ddbs3_key} from dynamodb: {e}\")\n","\n","# print(follows_aggregation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fFtjUNf-rbR","executionInfo":{"status":"ok","timestamp":1739769927210,"user_tz":300,"elapsed":269,"user":{"displayName":"Tyler Crews","userId":"08967755967182590100"}},"outputId":"3965c40c-db8d-4976-f63e-052301259680"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["ddb response: {'ResponseMetadata': {'RequestId': '42OTEB3DAR77C6OU2509G1UP27VV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Mon, 17 Feb 2025 05:25:26 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': '42OTEB3DAR77C6OU2509G1UP27VV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '2745614147'}, 'RetryAttempts': 0}}\n","WARNING - found no items in this key, runs may have failed yesterday\n"]}]},{"cell_type":"code","source":["# now we've aggregated all the values, so we just need to put that into s3\n","aggregate_list = list(follows_aggregation)\n","try:\n","  s3.put_object(\n","      Bucket=S3_BUCKET,\n","      Key=ddbs3_key,\n","      Body=json.dumps(aggregate_list),\n","      ContentType=\"application/json\"\n","  )\n","  logging_aggregator(f'successfully aggregated follows from {ddbs3_key}')\n","except ClientError as e:\n","    print(f\"ERROR - failed to upload object to s3: {e}\")\n","    logging_aggregator(f\"ERROR - failed to upload object to s3: {e}\")"],"metadata":{"id":"ClryEvizEoxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0ptAHP5tYLgy"},"execution_count":null,"outputs":[]}]}